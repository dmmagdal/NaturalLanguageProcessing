# bert_faiss.py
# Build a news recommendation model based on topic with Microsoft News
# Dataset (MIND).
# Source (Medium): https://medium.com/mlearning-ai/build-news-
# recommendation-model-using-python-bert-and-faiss-10ea8c65e6c
# Source (Collab): https://colab.research.google.com/drive/
# 1uuQaagWNh7gexSQhchpOgGyPKYK2e6SU#scrollTo=ooBElUsT53JO
# Source (Medium): https://towardsdatascience.com/billion-scale-
# semantic-similarity-search-with-faiss-sbert-c845614962e2
# Source (Medium): https://medium.com/analytics-vidhya/building-a-
# movie-recommendation-using-faiss-60d65b104dda
# Source (MIND dataset): https://msnews.github.io/index.html
# Python 3.7
# Tensorflow 2.4.0
# Window/MacOS/Linux


import copy
import time
import random
import warnings
import faiss
import pandas as pd
import numpy as np
from re import sub
# import plotly
# import plotly.io as pio
# import plotly.express as px
# import matplotlib.pyplot as plt
import torch
import transformers
from tqdm import tqdm


def main():
	# This system will measure semantic similarity between the current
	# news item (what the user is reading) and the rest of the items in
	# the index. This semantic similarity is different from the
	# term-matching based methods such as TF-IDF. 
	# TF-IDF (Term Frequency - Inverse Document Frequency) is commonly
	# used to create document vectors. The algorithm does not take word
	# order into account. Instead, a bag-of-words approach is used
	# where each term recieves a weight corresponding to its frequency
	# in a document. This assigns large weights to terms appearing
	# infrequently in the corpus but often in a certain document, which
	# potentially are more representative of the document than more
	# common words.
	# Then, a similarity score function such as cosine similarity is
	# used to calculate the difference between two vectors.
	# Semantic similarity of sentences is based on the meanings of the
	# words and the syntax of sentences. If two sentences are similar,
	# the structural relations between words may be or may not be
	# similar.

	# News Recommendation Dataset
	# The Microsoft News Dataset (MIND) is a large-scale dataset for
	# news recommendation research. It was collected from anonymized 
	# behavior logs of Microsoft News website. The mission of MIND is
	# to serve as a benchmark for news recommendation and facilitate
	# the research in new recommendation and recommender systems area.
	# MIND contains about 160k English news articles and more than 15
	# million impression logs generated by 1 million users. Every news
	# article contains rich textual content including title, abstract,
	# body, category, and entities. Each impression log contains the
	# click events, non-clicked events, and historical news click
	# behaviors of this user before this impression. To protect user
	# privacy, each user was de-linked from the production system when
	# securely hashed into an anonymized ID.
	# The dataset contains the following files:
	# -> behaviors: This includes click histories and impression logs
	#	of users (who clicked what).
	# -> entity embedding: 100-dimensional embeddings of the entities
	#	and relations learned from the subgraph.
	# -> news: The information of news articles.
	# -> relation embedding: 100-dimensional embeddings of the entities
	#	and relations learned from the subgraph.

	# Word Embeddings
	# Word embeddings are the basis of deep learning for NLP. Word
	# embeddings (word2vec, GloVe) are often pre-trained on text corpus
	# from co-occurance statistics. Word embeddings are applied in a
	# context free manner. For example, the bank word in these two
	# sentences has the same embeddings:
	# "open a bank account", "on the river bank"
	# This is a problem in NLP, because some words have totally
	# different meanings when found in different context as shown in
	# the example above.
	# One solution to that is to train contextual representations on
	# text corpus. One of the most successful solutions in contextual
	# embeddings model is BERT.

	# BERT Model
	# BERT, which stands for Bidirectional Encoder Representations from
	# Transformers, is based on Transformers, a deep learning model in
	# which every output element is connected to every input element,
	# and the weightings between them are dynamically calculated based
	# upon their connection.
	# The main idea of BERT is to mask k% of the input while training
	# and the model tries to predicted the masked ones, usually k=15%.
	# We will use Huggingface Transformers (pretrained BERT model) to
	# create sentence embeddings for the news items.

	# The Method
	# Unzip the news data and load it into pandas dataframe.
	data = pd.read_csv('news.tsv', header=None, sep='\t')
	data.columns = [
		'News ID', 'Category', 'SubCategory', 'Title', 'Abstract',
		'URL', 'Title Entities', 'Abstract Entities'
	]
	print(data.sample(10))

	# Plot the categories of the data.
	index_map = generate_mapping_data(data)

	c = data[['Category', 'SubCategory']].value_counts()
	index = []
	for i in c.index:
		index.append(np.array(i))
	index = np.array(index)

	df = pd.DataFrame(
		columns=['Category', 'Sub Category', 'Values']
	)
	df['Category'] = index[:, 0]
	df['Sub Category'] = index[:, 1]
	df['Values'] = c.values
	# px.bar(
	# 	data_frame=df, x='Category', y='Values', color='Sub Category'
	# )

	# Perform some data cleaning by dropping nan and removing
	# duplicated titles and titles with only 4 words in them.
	data.dropna(inplace=True)
	print("The number of articles before processing:", len(data))
	data.drop_duplicates(subset=['Title'], inplace=True)
	print("The number of articles after processing:", len(data))

	print("The number of articles before processing:", len(data))
	data = data[data['Title'].apply((lambda x: len(x.split()) >= 4))]
	print("The number of articles after processing:", len(data))

	# Download and load the BERT model.
	model_name = 'bert-base-uncased'
	model = transformers.BertModel.from_pretrained(model_name)
	tokenizer = transformers.BertTokenizer.from_pretrained(
		model_name, do_lower_case=True
	)


	# Create embeddings for a given sentence (title in this case).
	def embedding_fn(model, text):
		with torch.no_grad():
			# Generate tokens.
			tokens = tokenizer.encode(text)

			# Expand dims.
			batch_tokens = np.expand_dims(tokens, axis=0)
			batch_tokens = torch.tensor(batch_tokens)

			# Generate embedding and return hidden state.
			return model(batch_tokens)[0]


	# The following functions are used to calculate the mean for a
	# given embeddings and the cosine similarity between two vectors of
	# embeddings.
	def compute_mean(embedding):
		if not isinstance(embedding, torch.Tensor):
			print('Embedding must be a torch.Tensor')
			return
		return embedding.mean(1)


	def compute_cosine_similarity(x1, x2):
		# Given two points in vector space, measure the cosine
		# distance.
		return cosine_similarity(x1, x2)


	def compute_distance(x1, x2):
		# Replace this with your own measure.
		return compute_cosine_measure(
			x1.detach().numpy(), x2.detach().numpy()
		)


	# Calculate the embeddings for all the titles in the dataframe.
	CHUNK_SIZE_EACH = 44_000


	def __embedding(text):
		return compute_mean(embedding_fn(model, text))


	def compute_bert_embeddings(dataframe_chunk, current_index,
			end_marker):
		np_chunk = __embedding(
			dataframe_chunk.loc[current_index * end_marker]['Title']
		).detach().numpy()
		#np_chunk = np_chunk.reshape(np_chunk.shape[1])

		for idx in range(1, end_marker):
			try:
				embedding = __embedding(
					dataframe_chunk.loc[
						(current_index * end_marker) + idx
					]['Title']
				).detach().numpy()
				#embedding = embedding.reshape(embedding.shape[1])
				np_chunk = np.append(np_chunk, embedding, axis=0)
				print('\r {}'.format(np_chunk.shape), end='')
			except Exception as e:
				print(e)
				np_chunk = np.append(
					np_chunk, np.zeros(shape=(1, 768)), axis=0
				)
				continue

		print(np_chunk.shape)
		np.savez_compressed(
			'title_{}'.format(current_index), a=np_chunk
		)


	def compute_embeddings_and_save(dataframe):
		n_rows = len(dataframe)

		chunk_sizes = n_rows // CHUNK_SIZE_EACH
		remaining = n_rows - chunk_sizes * CHUNK_SIZE_EACH

		for i in range(1):
			compute_bert_embeddings(
				dataframe[i * CHUNK_SIZE_EACH: (i * CHUNK_SIZE_EACH) + CHUNK_SIZE_EACH],
				i, CHUNK_SIZE_EACH
			)


	# After creating the embeddings and saving them, they can be loaded
	# with:
	compute_embeddings_and_save(data)
	embeddings = np.load('title_0.npz')['a']
	print(embeddings.shape)

	# Create Index of Embeddings using FAISS
	# This index of embeddings will be used to look up the most similar
	# articles.
	n_dimensions = embeddings.shape[1] # Number of dimensions (764)

	# Create an index of type FlatL2. There ar emany kinds of indexes
	# you can refer to in the faiss repo.
	fastIndex = faiss.IndexFlatL2(n_dimensions) 
	fastIndex.add(embeddings.astype('float32')) # Add the embeddings
	# vector to faiss index, it should be of dtype 'float32'


	# Search the Index
	# To create a lookup function, we will create this function which
	# will take the index, query text, model, and top k values to
	# return as parameters.
	def do_faiss_lookup(fastIndex, query_text, model, top_k):
		n_embeddings = embeddings.shape[0]
		embedding_q = compute_mean(embedding_fn(
			model, query_text
		)).detach().numpy()

		# Let it be float32
		embedding_q = embedding_q.astype('float32')

		# Peform the search
		st = time.time()
		matched_em, matched_indexes = fastIndex.search(
			embedding_q, top_k
		) # Returns matched vectors and their respective indexes. We
		# are only interested in the indexes.

		# Indexes are already sorted with respect to closest match.
		et = time.time()
		return et - st, matched_indexes[0]


	# Example of a query and its similar articles.
	requested_query = "Anthony interviews Santa"
	faiss_time, top_indexes = do_faiss_lookup(
		fastIndex, requested_query, model, 20
	)
	print("Faiss index lookup time: ", faiss_time, " seconds")


	def index_to_title(indexes):
		for i, idx in enumerate(indexes):
			print("{}, {}".format(i, index_map[idx]['Title']))


	index_to_title(top_indexes)

	# Final remark is that this kind of recommendation system is
	# called a contextual system, which takes into consideraton only
	# content-similar articles. Another kind of recommendation is
	# personalized ones, where articles are recommended based on what
	# the users may be interested in.

	# Exit the program.
	exit(0)


def generate_mapping_data(dataframe):
	index_map = {}

	for index, row in dataframe.iterrows():
		index_map[index] = {
			'News ID': row['News ID'],
			'Title': row['Title'],
			'Abstract': row['Abstract'],
			'Category': row['Category'],
			'SubCategory': row['SubCategory'],
		}
	return index_map


if __name__ == '__main__':
	main()